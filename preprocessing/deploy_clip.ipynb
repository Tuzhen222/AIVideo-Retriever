{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install open_clip_torch faiss-cpu pyngrok torchscale","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, io, gc, time, asyncio\nimport torch, numpy as np\nfrom PIL import Image\nfrom queue import Queue\nfrom threading import Thread\nfrom typing import List\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom fastapi import FastAPI, File, UploadFile\nfrom pydantic import BaseModel\nimport uvicorn\nfrom pyngrok import ngrok\n\ntry:\n    import nest_asyncio\n    nest_asyncio.apply()\nexcept:\n    pass\n\n\n# =========================\n# CONFIG\n# =========================\nCLIP_MODEL_NAME = \"ViT-H-14-378-quickgelu\"\nCLIP_PRETRAINED = \"dfn5b\"\n\nBIGG_MODEL_NAME = \"ViT-bigG-14\"\nBIGG_PRETRAINED = \"laion2b_s39b_b160k\"\n\nBEIT3_REPO = \"Quintu/beit3\"\n\nCLIP_DEVICE = \"cuda:0\"\nBEIT3_DEVICE = \"cuda:0\"\nBIGG_DEVICE = \"cuda:1\"\n\nPORT = 7000\nMAX_BATCH = 24\nMAX_WAIT = 0.01\n\nNGROK_AUTH_TOKEN = \"36TVMtp50wfQnR4xVZ5allqmD76_oCvuKuUAZfPy5dB69VmJ\"\n\n\n# =========================\n# LOAD CLIP H/14\n# =========================\nimport open_clip\n\nclip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n    CLIP_MODEL_NAME,\n    pretrained=CLIP_PRETRAINED,\n    device=CLIP_DEVICE,\n)\nclip_model.eval()\n\nprint(f\"‚úÖ CLIP {CLIP_MODEL_NAME} loaded on {CLIP_DEVICE}\")\n\n# =========================\n# LOAD CLIP bigG\n# =========================\nprint(f\"üöÄ Loading CLIP {BIGG_MODEL_NAME} ({BIGG_PRETRAINED}) on {BIGG_DEVICE}...\")\nbigg_model, _, _ = open_clip.create_model_and_transforms(\n    model_name=BIGG_MODEL_NAME,\n    pretrained=BIGG_PRETRAINED,\n    device=BIGG_DEVICE,\n)\nbigg_model = bigg_model.to(device=BIGG_DEVICE, dtype=torch.float16).eval()\nbigg_tokenizer = open_clip.get_tokenizer(BIGG_MODEL_NAME)\nprint(\"‚úÖ CLIP bigG ready.\\n\")\n\n\n# =========================\n# LOAD BEiT3 (TEXT ONLY)\n# =========================\nfrom huggingface_hub import hf_hub_download\nfrom transformers import XLMRobertaTokenizer\nfrom timm.models.layers import trunc_normal_\n\n\nclass BEiT3QdrantSearcher:\n    class _BEiT3Wrapper(nn.Module):\n        def __init__(self, args):\n            super().__init__()\n            from torchscale.model.BEiT3 import BEiT3\n            self.beit3 = BEiT3(args)\n            self.apply(self._init_weights)\n\n        def _init_weights(self, m):\n            if isinstance(m, nn.Linear):\n                trunc_normal_(m.weight, std=.02)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    class _BEiT3ForRetrieval(_BEiT3Wrapper):\n        def __init__(self, args):\n            super().__init__(args)\n            d = args.encoder_embed_dim\n            self.language_head = nn.Linear(d, d, bias=False)\n            self._init_weights(self.language_head)\n\n        @torch.no_grad()\n        def encode_text(self, ids, mask):\n            out = self.beit3(\n                textual_tokens=ids,\n                visual_tokens=None,\n                text_padding_position=mask,\n            )\n            x = self.language_head(out[\"encoder_out\"][:, 0])\n            return F.normalize(x, dim=-1)\n\n    @staticmethod\n    def _cfg():\n        from torchscale.architecture.config import EncoderConfig\n\n        return EncoderConfig(\n            img_size=384,\n            patch_size=16,\n            vocab_size=64010,\n            multiway=True,\n            normalize_output=True,\n            no_output_layer=True,\n            encoder_embed_dim=1024,\n            encoder_layers=24,\n            encoder_attention_heads=16,\n            encoder_ffn_embed_dim=4096,\n        )\n\n    def __init__(self, repo=BEIT3_REPO):\n        self.repo = repo\n        self.tokenizer = None\n        self.model = None\n\n    def load(self, device):\n        ckpt_path = hf_hub_download(self.repo, \"beit3_large_patch16_384_coco_retrieval.pth\")\n        spm_path = hf_hub_download(self.repo, \"beit3.spm\")\n\n        self.tokenizer = XLMRobertaTokenizer(spm_path, use_fast=False)\n        args = self._cfg()\n        self.model = self._BEiT3ForRetrieval(args).to(device)\n\n        state = torch.load(ckpt_path, map_location=\"cpu\")\n        self.model.load_state_dict(state.get(\"model\", state), strict=False)\n        self.model.eval()\n\n        print(f\"‚úÖ BEiT3 loaded on {device}\")\n\n    @torch.no_grad()\n    def encode_text(self, texts: List[str]) -> np.ndarray:\n        enc = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\",\n        )\n        ids = enc[\"input_ids\"].to(self.model.language_head.weight.device)\n        mask = (enc[\"attention_mask\"] == 0).bool().to(ids.device)\n        out = self.model.encode_text(ids, mask)\n        return out.cpu().numpy()\n\n\nbeit3 = BEiT3QdrantSearcher()\nbeit3.load(BEIT3_DEVICE)\n\n\n# =========================\n# QUEUES + WORKERS (AUTO-BATCH FOR SINGLE REQUESTS)\n# =========================\nclip_img_q = Queue()\nclip_txt_q = Queue()\nbeit3_txt_q = Queue()\nbigg_txt_q = Queue()\n\n\ndef run_worker(q: Queue, fn):\n    \"\"\"\n    Generic worker:\n      - collects requests into `batch`\n      - runs `fn(batch)` once\n      - splits output row-wise back to callbacks\n    \"\"\"\n    def w():\n        while True:\n            batch, cbs = [], []\n\n            x, cb = q.get()\n            batch.append(x)\n            cbs.append(cb)\n\n            t0 = time.time()\n            while len(batch) < MAX_BATCH and (time.time() - t0) < MAX_WAIT:\n                try:\n                    x, cb = q.get_nowait()\n                    batch.append(x)\n                    cbs.append(cb)\n                except Exception:\n                    break\n\n            out = fn(batch)  # np.array (B, dim)\n            # split row-wise\n            for o, cb in zip(out, cbs):\n                cb(o)\n\n    Thread(target=w, daemon=True).start()\n\n\n# Image: B is list[tensor (1,C,H,W)]\nrun_worker(\n    clip_img_q,\n    lambda B: F.normalize(\n        clip_model.encode_image(torch.cat(B).to(CLIP_DEVICE)),\n        dim=-1,\n    ).detach().cpu().numpy(),\n)\n\n# CLIP text (SINGLE): B is list[token_tensor (1,seq)]\nrun_worker(\n    clip_txt_q,\n    lambda B: F.normalize(\n        clip_model.encode_text(torch.cat(B).to(CLIP_DEVICE)),\n        dim=-1,\n    ).detach().cpu().numpy(),\n)\n\n# BEiT3 text (SINGLE): B is list[str]\nrun_worker(\n    beit3_txt_q,\n    lambda B: beit3.encode_text(B),\n)\n\n# bigG text (SINGLE): B is list[str]\nrun_worker(\n    bigg_txt_q,\n    lambda B: F.normalize(\n        bigg_model.encode_text(bigg_tokenizer(B).to(BIGG_DEVICE)),\n        dim=-1,\n    ).detach().cpu().numpy(),\n)\n\n\n# =========================\n# FASTAPI APP\n# =========================\napp = FastAPI()\n\n\n# =========================\n# REQUEST/RESPONSE MODELS\n# =========================\nclass TextReq(BaseModel):\n    text: str\n\n\nclass BatchTextReq(BaseModel):\n    texts: List[str]\n\n\nclass EmbeddingResponse(BaseModel):\n    model: str\n    embedding: List[float]\n    dimension: int\n\n\nclass BatchEmbeddingResponse(BaseModel):\n    model: str\n    embeddings: List[List[float]]\n    dimension: int\n    count: int\n\n\n# =========================\n# SINGLE REQUEST ENDPOINTS\n# =========================\n@app.post(\"/embedding/clip/image\", response_model=EmbeddingResponse)\nasync def clip_img(file: UploadFile = File(...)):\n    pil = Image.open(io.BytesIO(await file.read())).convert(\"RGB\")\n    x = clip_preprocess(pil).unsqueeze(0)  # (1,C,H,W)\n\n    loop = asyncio.get_running_loop()\n    fut = loop.create_future()\n\n    clip_img_q.put((x, lambda r: loop.call_soon_threadsafe(fut.set_result, r)))\n    v = await fut  # 1D array (dim,)\n\n    v = v.astype(\"float32\")\n    return EmbeddingResponse(\n        model=\"clip-image\",\n        embedding=v.tolist(),\n        dimension=len(v),\n    )\n\n\n@app.post(\"/embedding/clip/text\", response_model=EmbeddingResponse)\nasync def clip_text(req: TextReq):\n    # tokenize single text ‚Üí (1,seq)\n    tok = open_clip.tokenize([req.text]).to(CLIP_DEVICE)\n\n    loop = asyncio.get_running_loop()\n    fut = loop.create_future()\n\n    clip_txt_q.put((tok, lambda r: loop.call_soon_threadsafe(fut.set_result, r)))\n    v = await fut  # 1D array\n\n    v = v.astype(\"float32\")\n    return EmbeddingResponse(\n        model=\"clip-text\",\n        embedding=v.tolist(),\n        dimension=len(v),\n    )\n\n\n@app.post(\"/embedding/beit3/text\", response_model=EmbeddingResponse)\nasync def beit3_text(req: TextReq):\n    loop = asyncio.get_running_loop()\n    fut = loop.create_future()\n\n    beit3_txt_q.put((req.text, lambda r: loop.call_soon_threadsafe(fut.set_result, r)))\n    v = await fut  # 1D array\n\n    v = v.astype(\"float32\")\n    return EmbeddingResponse(\n        model=\"beit3-text\",\n        embedding=v.tolist(),\n        dimension=len(v),\n    )\n\n\n@app.post(\"/embedding/bigg/text\", response_model=EmbeddingResponse)\nasync def bigg_text(req: TextReq):\n    loop = asyncio.get_running_loop()\n    fut = loop.create_future()\n\n    bigg_txt_q.put((req.text, lambda r: loop.call_soon_threadsafe(fut.set_result, r)))\n    v = await fut  # 1D array\n\n    v = v.astype(\"float32\")\n    return EmbeddingResponse(\n        model=\"bigg-text\",\n        embedding=v.tolist(),\n        dimension=len(v),\n    )\n\n\n# =========================\n# BATCH ENDPOINTS\n#  (KH√îNG D√ôNG QUEUE, G·ªåI MODEL TR·ª∞C TI·∫æP)\n# =========================\n@app.post(\"/embedding/clip/text/batch\", response_model=BatchEmbeddingResponse)\nasync def clip_text_batch(req: BatchTextReq):\n    \"\"\"\n    Batch text embedding cho CLIP.\n    G·ªçi model tr·ª±c ti·∫øp (kh√¥ng d√πng queue) ƒë·ªÉ tr√°nh conflict v·ªõi auto-batch.\n    \"\"\"\n    texts = req.texts\n    if not texts:\n        return BatchEmbeddingResponse(\n            model=\"clip-text\",\n            embeddings=[],\n            dimension=0,\n            count=0,\n        )\n\n    tok = open_clip.tokenize(texts).to(CLIP_DEVICE)\n    with torch.no_grad():\n        feats = clip_model.encode_text(tok)\n        feats = F.normalize(feats, dim=-1)\n\n    v = feats.detach().cpu().numpy().astype(\"float32\")  # (B,dim)\n    embeddings = v.tolist()\n\n    dim = v.shape[1] if v.size > 0 else 0\n    return BatchEmbeddingResponse(\n        model=\"clip-text\",\n        embeddings=embeddings,\n        dimension=dim,\n        count=len(embeddings),\n    )\n\n\n@app.post(\"/embedding/beit3/text/batch\", response_model=BatchEmbeddingResponse)\nasync def beit3_text_batch(req: BatchTextReq):\n    \"\"\"\n    Batch text embedding cho BEiT3.\n    D√πng encode_text native (ƒë√£ batch internally).\n    \"\"\"\n    texts = req.texts\n    if not texts:\n        return BatchEmbeddingResponse(\n            model=\"beit3-text\",\n            embeddings=[],\n            dimension=0,\n            count=0,\n        )\n\n    embeddings_array = beit3.encode_text(texts)  # (B,dim)\n    embeddings_array = embeddings_array.astype(\"float32\")\n    embeddings = embeddings_array.tolist()\n\n    dim = embeddings_array.shape[1] if embeddings_array.size > 0 else 0\n    return BatchEmbeddingResponse(\n        model=\"beit3-text\",\n        embeddings=embeddings,\n        dimension=dim,\n        count=len(embeddings),\n    )\n\n\n@app.post(\"/embedding/bigg/text/batch\", response_model=BatchEmbeddingResponse)\nasync def bigg_text_batch(req: BatchTextReq):\n    \"\"\"\n    Batch text embedding cho CLIP bigG.\n    G·ªçi model tr·ª±c ti·∫øp (kh√¥ng d√πng queue).\n    \"\"\"\n    texts = req.texts\n    if not texts:\n        return BatchEmbeddingResponse(\n            model=\"bigg-text\",\n            embeddings=[],\n            dimension=0,\n            count=0,\n        )\n\n    tok = bigg_tokenizer(texts).to(BIGG_DEVICE)\n    with torch.no_grad():\n        feats = bigg_model.encode_text(tok)\n        feats = F.normalize(feats, dim=-1)\n\n    v = feats.detach().cpu().numpy().astype(\"float32\")  # (B,dim)\n    embeddings = v.tolist()\n    dim = v.shape[1] if v.size > 0 else 0\n\n    return BatchEmbeddingResponse(\n        model=\"bigg-text\",\n        embeddings=embeddings,\n        dimension=dim,\n        count=len(embeddings),\n    )\n\n\n# =========================\n# HEALTH CHECK\n# =========================\n@app.get(\"/health\")\nasync def health_check():\n    return {\n        \"status\": \"healthy\",\n        \"models\": {\n            \"clip\": CLIP_MODEL_NAME,\n            \"beit3\": \"BEiT3 Large\",\n            \"bigg\": BIGG_MODEL_NAME,\n        },\n        \"devices\": {\n            \"clip\": CLIP_DEVICE,\n            \"beit3\": BEIT3_DEVICE,\n            \"bigg\": BIGG_DEVICE,\n        },\n        \"batch_support\": True,\n        \"max_batch_size\": MAX_BATCH,\n    }\n\n\n# =========================\n# MAIN\n# =========================\nif __name__ == \"__main__\":\n    print(\"=\" * 80)\n    print(\"üöÄ Video Retrieval Model Server\")\n    print(\"=\" * 80)\n    print(\"üì¶ Models loaded:\")\n    print(f\"   - CLIP: {CLIP_MODEL_NAME} on {CLIP_DEVICE}\")\n    print(f\"   - BEiT3: Large on {BEIT3_DEVICE}\")\n    print(f\"   - CLIP bigG: {BIGG_MODEL_NAME} on {BIGG_DEVICE}\")\n    print(f\"‚ö° Auto-batch (queue) for SINGLE endpoints, MAX_BATCH = {MAX_BATCH}\")\n    print(f\"üåê Port: {PORT}\")\n    print(\"=\" * 80)\n\n    if NGROK_AUTH_TOKEN:\n        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n        public_url = ngrok.connect(PORT).public_url\n        print(f\"üîó Public URL: {public_url}\")\n        print(\"=\" * 80)\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=PORT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T10:08:32.830673Z","iopub.execute_input":"2025-11-13T10:08:32.831154Z","iopub.status.idle":"2025-11-13T10:12:00.156029Z","shell.execute_reply.started":"2025-11-13T10:08:32.831126Z","shell.execute_reply":"2025-11-13T10:12:00.154905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !ps aux | grep python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:09:34.351713Z","iopub.execute_input":"2025-11-11T13:09:34.352031Z","iopub.status.idle":"2025-11-11T13:09:34.653600Z","shell.execute_reply.started":"2025-11-11T13:09:34.352005Z","shell.execute_reply":"2025-11-11T13:09:34.652748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !kill -9 48","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:09:36.725371Z","iopub.execute_input":"2025-11-11T13:09:36.726233Z","execution_failed":"2025-11-11T13:07:14.476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
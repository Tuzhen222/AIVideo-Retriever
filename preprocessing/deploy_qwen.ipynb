{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyngrok ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T10:17:54.968669Z","iopub.execute_input":"2025-11-13T10:17:54.968913Z","iopub.status.idle":"2025-11-13T10:17:59.438726Z","shell.execute_reply.started":"2025-11-13T10:17:54.968889Z","shell.execute_reply":"2025-11-13T10:17:59.437888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, io, time, asyncio\nimport torch, numpy as np\nfrom queue import Queue\nfrom threading import Thread\nfrom typing import List\nimport torch.nn.functional as F\nfrom fastapi import FastAPI\nfrom fastapi import UploadFile, File\nfrom pydantic import BaseModel\nimport uvicorn\nfrom pyngrok import ngrok\nimport nest_asyncio\nnest_asyncio.apply()\nfrom transformers import AutoTokenizer, AutoModel\n\n\nQWEN_MODEL_NAME = \"Qwen/Qwen3-Embedding-8B\"\nQWEN_DEVICE = \"cuda:0\"\nPORT = 7005         \nMAX_BATCH = 24\nMAX_WAIT = 0.01\nNGROK_AUTH_TOKEN = \"35Q4PzgSja5h0Vo3eJHH1Lf2sdn_5VjWqbgPk3NSsyXLeCuQ2\"  \n\n\nprint(\"ðŸ“¥ Loading Qwen tokenizer...\")\nqwen_tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_NAME, trust_remote_code=True)\nprint(\"âš¡ Loading Qwen3-Embedding-8B...\")\nqwen_model = AutoModel.from_pretrained(\n    QWEN_MODEL_NAME,\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": QWEN_DEVICE}\n).eval()\nprint(\"âœ… Qwen embedding model ready.\\n\")\n\n\nqwen_txt_q = Queue()\n\ndef run_worker(q, fn):\n    def w():\n        while True:\n            batch, cbs = [], []\n\n            x, cb = q.get()\n            batch.append(x); cbs.append(cb)\n            t0 = time.time()\n\n            while len(batch) < MAX_BATCH and (time.time() - t0) < MAX_WAIT:\n                try:\n                    x, cb = q.get_nowait()\n                    batch.append(x); cbs.append(cb)\n                except:\n                    break\n\n            out = fn(batch)\n            for o, cb in zip(out, cbs):\n                cb(o)\n\n    Thread(target=w, daemon=True).start()\n\n\nrun_worker(\n    qwen_txt_q,\n    lambda B: F.normalize(\n        qwen_model(\n            **qwen_tokenizer(\n                B,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=128\n            ).to(QWEN_DEVICE)\n        ).last_hidden_state[:, 0, :], \n        dim=-1\n    ).detach().cpu().numpy()\n)\n\n\napp = FastAPI()\n\n# ==============================================================================\n# REQUEST/RESPONSE MODELS\n# ==============================================================================\nclass TextReq(BaseModel):\n    text: str\n\nclass BatchTextReq(BaseModel):\n    texts: List[str]\n\nclass EmbeddingResponse(BaseModel):\n    model: str\n    embedding: List[float]\n    dimension: int\n\nclass BatchEmbeddingResponse(BaseModel):\n    model: str\n    embeddings: List[List[float]]\n    dimension: int\n    count: int\n\n\n# ==============================================================================\n# SINGLE REQUEST ENDPOINT\n# ==============================================================================\n@app.post(\"/embedding/qwen/text\")\nasync def qwen_text(req: TextReq):\n    \"\"\"\n    Single text embedding for IC model (Qwen).\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    fut = loop.create_future()\n\n    qwen_txt_q.put((req.text, lambda r: loop.call_soon_threadsafe(fut.set_result, r)))\n    v = await fut\n    v = v.astype(\"float32\")\n\n    return {\n        \"model\": \"qwen-text\",\n        \"embedding\": v.tolist(),\n        \"dimension\": len(v)\n    }\n\n\n# ==============================================================================\n# BATCH REQUEST ENDPOINT\n# ==============================================================================\n@app.post(\"/embedding/qwen/text/batch\")\nasync def qwen_text_batch(req: BatchTextReq):\n    \"\"\"\n    Batch text embedding for IC model (Qwen).\n    Processes multiple texts in one GPU forward pass.\n    Perfect for query augmentation (Q0, Q1, Q2).\n    \"\"\"\n    texts = req.texts\n    if not texts:\n        return {\"model\": \"qwen-text\", \"embeddings\": [], \"dimension\": 0, \"count\": 0}\n    \n    # Process all texts in batch\n    loop = asyncio.get_running_loop()\n    fut = loop.create_future()\n    qwen_txt_q.put((texts, lambda r: loop.call_soon_threadsafe(fut.set_result, r)))\n    \n    v = await fut  # Shape: (batch_size, embedding_dim)\n    v = v.astype(\"float32\")\n    \n    # Convert to list of embeddings\n    embeddings = [emb.tolist() for emb in v]\n    \n    return {\n        \"model\": \"qwen-text\",\n        \"embeddings\": embeddings,\n        \"dimension\": len(embeddings[0]) if embeddings else 0,\n        \"count\": len(embeddings)\n    }\n\n\n# ==============================================================================\n# HEALTH CHECK\n# ==============================================================================\n@app.get(\"/health\")\nasync def health_check():\n    return {\n        \"status\": \"healthy\",\n        \"model\": QWEN_MODEL_NAME,\n        \"device\": QWEN_DEVICE,\n        \"batch_support\": True,\n        \"max_batch_size\": MAX_BATCH\n    }\n\n\nif __name__ == \"__main__\":\n    print(\"=\" * 80)\n    print(\"ðŸš€ IC Model Server (Qwen3-Embedding-8B)\")\n    print(\"=\" * 80)\n    print(f\"ðŸ“¦ Model: {QWEN_MODEL_NAME}\")\n    print(f\"ðŸŽ¯ Device: {QWEN_DEVICE}\")\n    print(f\"âš¡ Batch processing enabled (max: {MAX_BATCH})\")\n    print(f\"ðŸŒ Port: {PORT}\")\n    print(\"=\" * 80)\n    \n    if NGROK_AUTH_TOKEN:\n        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n        public_url = ngrok.connect(PORT).public_url\n        print(f\"ðŸ”— NGROK URL: {public_url}\")\n        print(\"=\" * 80)\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=PORT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T10:17:59.440915Z","iopub.execute_input":"2025-11-13T10:17:59.441172Z","execution_failed":"2025-11-13T10:23:18.288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}